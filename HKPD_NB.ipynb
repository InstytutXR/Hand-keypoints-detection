{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision import image as im\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import mse_loss\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'path_to_your_dataset'    # You need to specify the path to your dataset\n",
    "                                # you can find many datasets out there. the best would be to annotate your own dataset, \n",
    "                                # but it's too time consuming. if like me, you are intrested to learn the dynamics \n",
    "                                # at the heart of computure vision, you can pick one of the datasets available out there.\n",
    "                                # i won't matter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the transformations to perform on your dataset\n",
    "transforms = get_transforms(do_flip=False, max_zoom=1.1, max_warp=0.01,max_rotate=45) \n",
    "# This function gets hand's coordinates from a json file. Very important!!, in fastai \"y\" is expected to be first before \"x\" \n",
    "def get_y_func(x):\n",
    "     pre, ext = os.path.splitext(x)\n",
    "     img = open_image(x)\n",
    "     coords = []\n",
    "     for k in json.load(open(pre + '.json'))['hand_keypoints']:\n",
    "        coords.append([k[1],k[0]]) # inverting x and y\n",
    "     return torch.tensor(coords,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting your data_block\n",
    "data = (PointsItemList.from_folder(path=path, extensions=['.jpg'])\n",
    "        .split_by_folder(train='train', valid='test') #  setting training and testing dataset folders paths\n",
    "        .label_from_func(get_y_func)  # using get_y_func() to get coordinates for each image\n",
    "        .transform(transforms,size=224, tfm_y=True, remove_out=False,  # very important!!!: setting remove_out to False, \n",
    "                                                                       # prevents from discarding coordinates that may \n",
    "                                                                       # disappear after data augmentation    \n",
    "                   padding_mode='border', resize_method=ResizeMethod.PAD)\n",
    "        .databunch(bs=8) # Setting your batch size. \n",
    "        .normalize(imagenet_stats)) # Normalizing the data to help the model converging faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reshapes the output tensor to (in our case) a 21x2 tensor, witch corresponds to the 21 hand coordinates   \n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the classification head and setting a custom head for regression:\n",
    "# A basic Flattening and downsizing to get the 21 final coordinates. \n",
    "head_reg = nn.Sequential(\n",
    "    Flatten(), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512*7*7, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 42),\n",
    "    Reshape(-1,21,2),\n",
    "    nn.Tanh())            # I added the Tanh function to keep the output in [-1,1] range to help the model converge faster.\n",
    "                          # you can remove it, it won't prevent the model from converging  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting a custom loss function using MSELoss class. We need to flatten the output tensor to be able \n",
    "# to compare it to the target (hence .view(-1)).  \n",
    "class MSELossFlat(nn.MSELoss):\n",
    "    def forward(self, input:Tensor, target:Tensor):\n",
    "     return super().forward(input.view(-1), target.view(-1)) \n",
    "\n",
    "mse_loss_flat = MSELossFlat()    #very important!!:initialazing the class\n",
    "\n",
    "learn = cnn_learner(data, models.resnet34,custom_head=head_reg, loss_func=mse_loss_flat) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
